{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ece08c2",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507bf78e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ROC AUC Score: 0.9682772488392346\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Define a function to prepare text data\n",
    "def prepare_text(text):\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return nltk.corpus.wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return nltk.corpus.wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return nltk.corpus.wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return nltk.corpus.wordnet.ADV\n",
    "        else:\n",
    "            return nltk.corpus.wordnet.NOUN\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = nltk.pos_tag(text)\n",
    "    lemma = []\n",
    "    for word, pos in text:\n",
    "        lemma.append(nltk.WordNetLemmatizer().lemmatize(word, pos=get_wordnet_pos(pos)))\n",
    "    return ' '.join(lemma)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('FinalBalancedDataset.csv')\n",
    "data = df.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# Prepare text data\n",
    "data[\"clean_tweets\"] = data[\"tweet\"].apply(prepare_text)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['clean_tweets'], data['Toxicity'], test_size=0.8, random_state=42)\n",
    "\n",
    "# Initialize and fit TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Multinomial Naive Bayes model\n",
    "model_bayes = MultinomialNB()\n",
    "model_bayes.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_pred_proba = model_bayes.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "final_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"Final ROC AUC Score:\", final_roc_auc)\n",
    "\n",
    "# Save the trained TfidfVectorizer\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "def classify_comment():\n",
    "    # Get input comment from entry widget\n",
    "    comment = entry_comment.get()\n",
    "    # Preprocess the comment\n",
    "    preprocessed_comment = prepare_text(comment)\n",
    "    # Transform the preprocessed comment using the TfidfVectorizer\n",
    "    tfidf_comment = tfidf_vectorizer.transform([preprocessed_comment])\n",
    "    # Predict toxicity using the trained model\n",
    "    toxicity_prob = model_bayes.predict_proba(tfidf_comment)[0][1]\n",
    "    # Display the result\n",
    "    if toxicity_prob >= 0.5:\n",
    "        result = \"Toxic\"\n",
    "    else:\n",
    "        result = \"Non-Toxic\"\n",
    "    messagebox.showinfo(\"Classification Result\", f\"The comment is {result}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e80c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is toxic.\n"
     ]
    }
   ],
   "source": [
    "# Text to predict\n",
    "text_to_predict = \"it was a bullshit\"\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = prepare_text(text_to_predict)\n",
    "\n",
    "# Transform the preprocessed text using the TfidfVectorizer\n",
    "tfidf_text = tfidf_vectorizer.transform([preprocessed_text])\n",
    "\n",
    "# Use the trained Multinomial Naive Bayes model to predict probabilities\n",
    "predicted_probability = model_bayes.predict_proba(tfidf_text)[0][1]\n",
    "\n",
    "# Determine if the text is toxic based on the predicted probability\n",
    "if predicted_probability >= 0.5:\n",
    "    print(\"The text is toxic.\")\n",
    "else:\n",
    "    print(\"The text is not toxic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d5398",
   "metadata": {},
   "source": [
    "# full project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61caca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Define a function to prepare text data\n",
    "def prepare_text(text):\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return nltk.corpus.wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return nltk.corpus.wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return nltk.corpus.wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return nltk.corpus.wordnet.ADV\n",
    "        else:\n",
    "            return nltk.corpus.wordnet.NOUN\n",
    "\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = nltk.pos_tag(text)\n",
    "    lemma = []\n",
    "    for word, pos in text:\n",
    "        lemma.append(nltk.WordNetLemmatizer().lemmatize(word, pos=get_wordnet_pos(pos)))\n",
    "    return ' '.join(lemma)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\harsh\\DSA\\FinalBalancedDataset.csv\")\n",
    "data = df.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# Prepare text data\n",
    "data[\"clean_tweets\"] = data[\"tweet\"].apply(prepare_text)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['clean_tweets'], data['Toxicity'], test_size=0.8, random_state=42)\n",
    "\n",
    "# Initialize and fit TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train Multinomial Naive Bayes model\n",
    "model_bayes = MultinomialNB()\n",
    "model_bayes.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Save the trained TfidfVectorizer\n",
    "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "def classify_comment():\n",
    "    # Get input comment from entry widget\n",
    "    comment = entry_comment.get()\n",
    "    # Preprocess the comment\n",
    "    preprocessed_comment = prepare_text(comment)\n",
    "    # Transform the preprocessed comment using the TfidfVectorizer\n",
    "    tfidf_comment = tfidf_vectorizer.transform([preprocessed_comment])\n",
    "    # Predict toxicity using the trained model\n",
    "    toxicity_prob = model_bayes.predict_proba(tfidf_comment)[0][1]\n",
    "    # Display the result\n",
    "    if toxicity_prob >= 0.5:\n",
    "        result = \"Toxic\"\n",
    "    else:\n",
    "        result = \"Non-Toxic\"\n",
    "    messagebox.showinfo(\"Classification Result\", f\"The comment is {result}.\")\n",
    "\n",
    "root = Tk()\n",
    "root.title(\"Login\")\n",
    "root.geometry('925x500+300+200')\n",
    "root.configure(bg=\"#fff\")\n",
    "root.resizable(False,False)\n",
    "\n",
    "def signin():\n",
    "    username = user.get()\n",
    "    password = code.get()\n",
    "\n",
    "    if username == 'admin' and password =='1234':\n",
    "        screen = Toplevel(root)\n",
    "        screen.title(\"App\")\n",
    "        screen.geometry(\"925x500+300+200\")\n",
    "        screen.config(bg='White')\n",
    "        Label(screen, text=\"Hello World\", bg='#fff', font=('Calibri(Body)')).pack(expand = True)\n",
    "\n",
    "        # GUI for comment classification\n",
    "        comment_frame = Frame(screen, bg=\"#fff\")\n",
    "        comment_frame.place(x=50, y=200)\n",
    "\n",
    "        Label(comment_frame, text=\"Enter Comment:\", bg=\"#fff\", font=(\"Calibri\", 12)).grid(row=0, column=0, padx=10, pady=10)\n",
    "\n",
    "        global entry_comment\n",
    "        entry_comment = Entry(comment_frame, width=50)\n",
    "        entry_comment.grid(row=0, column=1, padx=10, pady=10)\n",
    "\n",
    "        classify_button = Button(comment_frame, text=\"Classify\", command=classify_comment)\n",
    "        classify_button.grid(row=1, columnspan=2, pady=10)\n",
    "\n",
    "        screen.mainloop()\n",
    "    elif username!= 'admin' and password != '1234':\n",
    "        messagebox.showerror(\"Invalid\", \"invalid username and password\")\n",
    "    elif password != '1234':\n",
    "        messagebox.showerror(\"Invalid\", \"invalid password\")\n",
    "    elif username != 'admin':\n",
    "        messagebox.showerror(\"Invalid\", \"invalid username\")\n",
    "\n",
    "img = PhotoImage(file=r\"C:\\Users\\harsh\\PycharmProjects\\pythonProject1\\login.png\")\n",
    "Label(root,image=img, bg=\"White\").place(x=50, y=50)\n",
    "\n",
    "frame = Frame(root, width=350, height=350, bg=\"White\" )\n",
    "frame.place(x=480, y=70)\n",
    "\n",
    "heading = Label(frame, text=\"Sign in\", fg=\"#57a1f8\",bg='White', font=('Microsoft YaHei UI Light', 23, 'bold'))\n",
    "heading.place(x=100,y=5)\n",
    "\n",
    "def on_enter(e):\n",
    "    user.delete(0, 'end')\n",
    "\n",
    "def on_leve(e):\n",
    "    name = user.get()\n",
    "    if name == '':\n",
    "        user.insert(0, 'Username')\n",
    "\n",
    "user = Entry(frame, width=25, fg='black', border=2, bg=\"White\", font=('Microsoft YaHei UI Light', 11))\n",
    "user.place(x=30, y=80)\n",
    "user.insert(0,'Username')\n",
    "user.bind('<FocusIn>', on_enter)\n",
    "user.bind('<FocusOut>',on_leve)\n",
    "Frame(frame,width=295, height=2, bg='black').place(x=25, y=107)\n",
    "\n",
    "def on_enter(e):\n",
    "    code.delete(0, 'end')\n",
    "\n",
    "def on_leve(e):\n",
    "    name = code.get()\n",
    "    if name == '':\n",
    "        code.insert(0, 'Password')\n",
    "\n",
    "code = Entry(frame, width=25, fg='black', border=2, bg=\"White\", font=('Microsoft YaHei UI Light', 11))\n",
    "code.place(x=30, y=150)\n",
    "code.insert(0,'Password')\n",
    "code.bind('<FocusIn>', on_enter)\n",
    "code.bind('<FocusOut>',on_leve)\n",
    "Frame(frame,width=295, height=2, bg='black').place(x=25, y=177)\n",
    "\n",
    "Button(frame, width=39, pady=7, text=\"Sign in\", bg='#57a1f8', fg='White', border=0, command= signin).place(x=35, y=204)\n",
    "label = Label(frame,text=\"Dont Have any account?\", fg='black', bg='White', font=('Microsoft YaHei UI Light',9))\n",
    "label.place(x=75, y=270)\n",
    "\n",
    "sign_up = Button(frame, width=6, text=\"Sign up\", border=0, bg='White', cursor='hand2', fg='#57a1f8')\n",
    "sign_up.place(x=215,y=270)\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
